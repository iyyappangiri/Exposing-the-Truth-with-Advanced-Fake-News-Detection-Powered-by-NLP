# Advanced English Fake News Detection System for Google Colab
# Uses two CSV files: Fake.csv and True.csv with columns: title, text, subject, date

# Install required libraries
!pip install pandas numpy regex joblib matplotlib seaborn wordcloud scikit-learn xgboost transformers nltk lime torch

import pandas as pd
import numpy as np
import re
import joblib
import os
import glob
import logging
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from xgboost import XGBClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, confusion_matrix, roc_curve, auc, roc_auc_score)
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import nltk
from lime.lime_text import LimeTextExplainer
from google.colab import files
from IPython.display import display
%matplotlib inline

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Set NLTK data path and download resources
nltk.data.path.append('/content/nltk_data')
try:
    nltk.download(['punkt', 'punkt_tab', 'stopwords', 'wordnet'], download_dir='/content/nltk_data', quiet=True)
    logger.info("NLTK resources downloaded successfully")
except Exception as e:
    logger.error(f"Error downloading NLTK resources: {e}")
    print(f"Error downloading NLTK resources: {e}")

# Enhanced text preprocessing for English
def preprocess_text(text):
    if not isinstance(text, str):
        return ''
    
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\@\w+|\#', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF]', '', text)
    text = re.sub(r'\d+', 'NUMBER', text)
    
    try:
        tokens = word_tokenize(text.lower())
    except Exception as e:
        logger.error(f"Tokenization error: {e}")
        return ''
    
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens 
             if token not in stopwords.words('english') and len(token) > 2]
    
    return ' '.join(tokens)

# Validate input text
def validate_input_text(text):
    if not text or len(text.strip()) < 10:
        return False, "Input text is too short or empty. Please provide a valid news article."
    return True, ""

# Load and prepare dataset from two CSV files
def load_dataset(fake_path, true_path, sample_size=1000):
    try:
        df_fake = pd.read_csv(fake_path, encoding='utf-8')
        df_true = pd.read_csv(true_path, encoding='utf-8')
        
        df_fake['label'] = 1
        df_true['label'] = 0
        
        if sample_size:
            df_fake = df_fake.sample(n=min(sample_size//2, len(df_fake)), random_state=42)
            df_true = df_true.sample(n=min(sample_size//2, len(df_true)), random_state=42)
        
        required_columns = ['title', 'text', 'subject', 'date']
        for df in [df_fake, df_true]:
            if not all(col in df.columns for col in required_columns):
                print(f"Error: Both CSV files must contain these columns: {required_columns}")
                return None
        
        df = pd.concat([df_fake, df_true], ignore_index=True)
        
        if not df['label'].isin([0,1]).all():
            print("Error: Labels must be binary (0 or 1)")
            return None
        
        df['title'] = df['title'].fillna('')
        df['text'] = df['text'].fillna('')
        
        df['combined_text'] = df['title'] + ' ' + df['text']
        
        df['cleaned_text'] = df['combined_text'].apply(preprocess_text)
        
        logger.info(f"Dataset loaded: {len(df)} articles")
        return df
    except FileNotFoundError as e:
        print(f"One or both dataset files not found: {e}")
        return None
    except Exception as e:
        print(f"Error loading dataset: {e}")
        return None

# Save models
def save_models(models, vectorizer, path='models/'):
    try:
        os.makedirs(path, exist_ok=True)
        joblib.dump(vectorizer, f'{path}vectorizer.pkl')
        for name, data in models.items():
            joblib.dump(data['model'], f'{path}{name.replace(" ", "_")}.pkl')
        logger.info("Models saved successfully")
    except Exception as e:
        logger.error(f"Error saving models: {e}")
        print(f"Error saving models: {e}")

# Load models
def load_models(path='models/'):
    try:
        vectorizer = joblib.load(f'{path}vectorizer.pkl')
        model_files = glob.glob(f'{path}*.pkl')
        models = {}
        for file in model_files:
            name = os.path.basename(file).replace('.pkl', '').replace('_', ' ')
            if name != 'vectorizer':
                models[name] = {'model': joblib.load(file)}
        if not models:
            print("Error: No pre-trained models found in the specified directory.")
            return None, None
        logger.info("Models loaded successfully")
        return models, vectorizer
    except Exception as e:
        logger.error(f"Error loading models: {e}")
        print(f"Error loading models: {e}")
        return None, None

# Train and evaluate multiple models
def train_models(df):
    if df is None:
        return None, None, None
    
    X = df['cleaned_text']
    y = df['label']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    vectorizer = TfidfVectorizer(max_features=3000, ngram_range=(1, 2))
    X_train_tfidf = vectorizer.fit_transform(X_train)
    X_test_tfidf = vectorizer.transform(X_test)

    models = {
        "Logistic Regression": LogisticRegression(max_iter=1000, class_weight='balanced'),
        "Random Forest": RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),
        "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
        "Ensemble": VotingClassifier(
            estimators=[
                ('lr', LogisticRegression(max_iter=1000)),
                ('rf', RandomForestClassifier(n_estimators=100)),
                ('xgb', XGBClassifier(use_label_encoder=False))
            ],
            voting='soft'
        )
    }

    results = {}
    best_model = None
    best_score = 0
    
    for i, (name, model) in enumerate(models.items()):
        print(f"Training {name} ({i+1}/{len(models)})...")
        model.fit(X_train_tfidf, y_train)
        
        y_pred = model.predict(X_test_tfidf)
        y_proba = model.predict_proba(X_test_tfidf)[:, 1]
        
        cv_scores = cross_val_score(model, X_train_tfidf, y_train, cv=5, scoring='f1')
        
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, zero_division=0),
            'recall': recall_score(y_test, y_pred, zero_division=0),
            'f1': f1_score(y_test, y_pred, zero_division=0),
            'roc_auc': roc_auc_score(y_test, y_proba),
            'cv_f1_mean': cv_scores.mean(),
            'cv_f1_std': cv_scores.std()
        }
        
        results[name] = {
            'model': model,
            'metrics': metrics,
            'y_pred': y_pred,
            'y_proba': y_proba,
            'y_test': y_test
        }
        
        if metrics['f1'] > best_score:
            best_score = metrics['f1']
            best_model = name
        
        logger.info(f"Trained {name}: F1 Score = {metrics['f1']:.3f}")
    
    return results, vectorizer, best_model

# Load pre-trained transformer model for English
def load_transformer_model():
    try:
        model_name = "distilbert-base-uncased-finetuned-sst-2-english"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(model_name)
        logger.info("Transformer model loaded successfully")
        return pipeline("text-classification", model=model, tokenizer=tokenizer)
    except Exception as e:
        logger.error(f"Error loading transformer model: {e}")
        print(f"Error loading transformer model: {e}")
        return None

# Explain model predictions using LIME
def explain_prediction(text, model, vectorizer):
    explainer = LimeTextExplainer(class_names=["True", "Fake"])
    def predict_proba(texts):
        return model.predict_proba(vectorizer.transform(texts))
    
    exp = explainer.explain_instance(text, predict_proba, num_features=10)
    return exp.as_list()

# Visualization functions
def plot_word_cloud(text, title):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title, fontsize=16)
    plt.axis('off')
    plt.show()

def plot_confusion_matrix(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['True', 'Fake'], yticklabels=['True', 'Fake'])
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix')
    plt.show()

def plot_roc_curve(y_true, y_proba):
    fpr, tpr, _ = roc_curve(y_true, y_proba)
    roc_auc = auc(fpr, tpr)
    
    plt.figure(figsize=(6, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc="lower right")
    plt.show()

# Main function for Colab
def main():
    print("üîç TruthGuard: English Fake News Detection")
    print("This system uses advanced NLP to detect fake news in English.")
    
    models = None
    vectorizer = None
    df = None
    transformer_model = load_transformer_model()
    best_model = None
    
    print("\nStep 1: Upload Fake.csv and True.csv")
    uploaded_files = files.upload()
    
    fake_path = None
    true_path = None
    for filename in uploaded_files.keys():
        if 'Fake.csv' in filename:
            fake_path = filename
        if 'True.csv' in filename:
            true_path = filename
    
    if fake_path and true_path:
        df = load_dataset(fake_path, true_path, sample_size=1000)
        if df is not None:
            print(f"Dataset loaded successfully: {len(df)} articles")
            print(f"Columns: {', '.join(df.columns)}")
            print("Class Distribution:")
            print(df['label'].value_counts())
    else:
        print("Error: Please upload both Fake.csv and True.csv")
        return
    
    action = input("\nStep 2: Do you want to (1) Train new models or (2) Load pre-trained models? Enter 1 or 2: ")
    
    if action == '1':
        if df is not None:
            print("\nTraining models...")
            results, vectorizer, best_model = train_models(df)
            if results:
                models = results
                save_models(models, vectorizer)
                print("\nModels trained and saved successfully!")
                
                print("\nModel Performance:")
                metrics_df = pd.DataFrame({
                    model: data['metrics'] for model, data in results.items()
                }).T
                display(metrics_df)
                print(f"Best model: {best_model}")
                
                for name, data in models.items():
                    print(f"\nVisualizations for {name}:")
                    plot_confusion_matrix(data['y_test'], data['y_pred'])
                    plot_roc_curve(data['y_test'], data['y_proba'])
        else:
            print("Error: Dataset not loaded. Cannot train models.")
    
    elif action == '2':
        models, vectorizer = load_models()
        if models and vectorizer:
            best_model = max(models.items(), key=lambda x: x[1].get('metrics', {}).get('f1', 0))[0]
            print("Pre-trained models loaded successfully!")
        else:
            print("Error: Could not load pre-trained models.")
    
    print("\nStep 3: Analyze a news article")
    news_text = input("Enter the news article text (or paste it here): ")
    
    if news_text.strip():
        is_valid, error_msg = validate_input_text(news_text)
        if not is_valid:
            print(f"Error: {error_msg}")
        else:
            print("\nAnalyzing text...")
            cleaned_text = preprocess_text(news_text)
            
            print("\nProcessed Text:")
            print(cleaned_text)
            
            if transformer_model:
                result = transformer_model(news_text[:512])[0]
                prediction = "Fake" if result['label'] == "NEGATIVE" else "True"
                confidence = result['score']
                
                print("\nTransformer Model Results:")
                print(f"Prediction: {prediction}")
                print(f"Confidence: {confidence:.2%}")
                
                plot_word_cloud(cleaned_text, "Word Cloud of Processed Text")
            
            if models and vectorizer:
                print("\nTraditional Model Comparison:")
                vectorized_text = vectorizer.transform([cleaned_text])
                best_model_obj = models[best_model]['model']
                trad_prediction = best_model_obj.predict(vectorized_text)[0]
                trad_proba = best_model_obj.predict_proba(vectorized_text)[0]
                
                print(f"Best Model ({best_model}) Prediction: {'Fake' if trad_prediction == 1 else 'True'}")
                print(f"Probability (True): {trad_proba[0]:.2%}")
                print(f"Probability (Fake): {trad_proba[1]:.2%}")
                
                print("\nExplanation (Traditional Model):")
                explanation = explain_prediction(cleaned_text, best_model_obj, vectorizer)
                print("Top features influencing this prediction:")
                for feature, weight in explanation:
                    print(f"{feature}: {weight:.3f}")
    
    if df is not None:
        print("\nStep 4: Data Exploration")
        print("\nDataset Preview:")
        display(df.head())
        
        if 'subject' in df.columns:
            print("\nSubject Distribution:")
            plt.figure(figsize=(10, 4))
            df['subject'].value_counts().plot(kind='bar')
            plt.xticks(rotation=45)
            plt.show()
        
        if 'date' in df.columns:
            try:
                df['date'] = pd.to_datetime(df['date'], errors='coerce')
                if df['date'].isna().all():
                    print("Warning: All dates are invalid or unparseable.")
                else:
                    print("\nArticles Over Time:")
                    time_df = df.set_index('date').resample('M').size()
                    time_df.plot(figsize=(10, 4))
                    plt.show()
            except Exception as e:
                print(f"Warning: Could not parse date column: {e}")

if __name__ == '__main__':
    main()
